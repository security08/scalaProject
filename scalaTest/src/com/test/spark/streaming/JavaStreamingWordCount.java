package com.test.spark.streaming;

import com.clearspring.analytics.util.Lists;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFlatMapFunction;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.storage.StorageLevel;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;
import scala.actors.threadpool.Arrays;

import java.util.List;
import java.util.regex.Pattern;

/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every 5 second.
 *
 * Usage: JavaStreamingWordCount <hostname> <port> <master> <1(default):exampleCode,2:flatMapToPair,3:window>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * After a context(JavaStreamingContext) is defined, you have to do the following.
 *    Define the input sources by creating input DStreams.
 *    Define the streaming computations by applying transformation and output operations to DStreams.
 *    Start receiving data and processing it using streamingContext.start().
 *    Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().
 *    The processing can be manually stopped using streamingContext.stop().
 *
 * Points to remember:
 *    Once a context has been started, no new streaming computations can be set up or added to it.
 *    Once a context has been stopped, it cannot be restarted.
 *    Only one StreamingContext can be active in a JVM at the same time.
 *    stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set optional parameter of stop() called stopSparkContext to false.
 *    A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.
 *
 * To run this on your local machine, you need to first run a Netcat server
 * `$ nc -lk 9999`
 * and then run the example
 * `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`
 *
 * reference: http://spark.apache.org/docs/latest/streaming-programming-guide.html
 * Created by kinwu on 2015/2/11.
 */
public class JavaStreamingWordCount {
    private static final Pattern SPACE = Pattern.compile("\\s+");

    public static void main(String[] args) {
        if (args.length < 4) {
            System.err.println("Usage: JavaStreamingWordCount <hostname> <port> <master> <1(default):exampleCode,2:flatMapToPair,3:window>");
            System.exit(1);
        }
        StreamingExamples.setStreamingLogLevels();

//        System.setProperty("hadoop.home.dir", "D:/openSource/hadoop-2.6.0");

        SparkConf conf = new SparkConf();
        conf.setAppName("JavaStreamingWordCount");
        conf.setMaster(args[2]);
        JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(5));

        // Create a JavaReceiverInputDStream on target ip:port and count the
        // words in input stream of \n delimited text (eg. generated by 'nc')
        // Note that no duplication in storage level only for running locally.
        // Replication necessary in distributed scenario for fault tolerance.
        JavaReceiverInputDStream<String> lines = jsc.socketTextStream(args[0],Integer.parseInt(args[1]),
                StorageLevel.MEMORY_ONLY_SER());

        JavaPairDStream<String,Integer> wordcount = null;

        switch (args[3]) {
            case "2" :
                System.out.println("2: run flatMapToPair");wordcount = flatMapToPair(lines); break;
            case "3" :
                System.out.println("3: run exampleCode"); wordcount = exampleCode(lines); break;
            default :
                System.out.println("default: reduceByKeyAndWindow");  wordcount = reduceByKeyAndWindow(lines);
        }

        //Prints first ten elements of every batch of data in a DStream on the driver node running the streaming application.
        wordcount.print();

        jsc.start();
        jsc.awaitTermination();
    }

    public  static JavaPairDStream<String,Integer> reduceByKeyAndWindow(JavaReceiverInputDStream<String> lines){
        // Count each word in each batch
        return  getWords(lines).mapToPair(getPairFunction()).
        //These two parameters(window and slide durations) must be multiples of the batch
        // interval of the source DStream(batchDuration in JavaStreamingContext)
                // Reduce last 15 seconds of data, every 10 seconds
         reduceByKeyAndWindow(getReduceFunc(), Durations.seconds(15), Durations.seconds(10));
    }


    /**
     * spark JavaNetworkWordCount example code
     * @param lines
     * @return
     */
    public static JavaPairDStream<String,Integer> exampleCode(JavaReceiverInputDStream<String> lines){
        return getWords(lines).mapToPair(getPairFunction()).reduceByKey(getReduceFunc());
    }

    public static JavaPairDStream<String,Integer> flatMapToPair(JavaReceiverInputDStream<String> lines){
        return lines.flatMapToPair(new PairFlatMapFunction<String, String ,Integer>(){
            @Override
            public Iterable<Tuple2<String, Integer>> call(String str){
                List<Tuple2<String, Integer>> tuple2List = Lists.newArrayList();
                String[] words = SPACE.split(str);
                for(String word : words){
                    tuple2List.add(new Tuple2<>(word,1));
                }
                return tuple2List;
            }

        }).
        //reduceByKey must be invoke as lines.flatMapToPair(...).reduceByKey(...);
        //if invoke by wordcount.reduceByKey(...) the output will be like :
        // (a,1)
        // (a,1)
        // but not:(a,2)
        reduceByKey(getReduceFunc());
    }

    /**
     * Split each line into words
     * @param lines
     * @return
     */
    public static JavaDStream<String> getWords(JavaReceiverInputDStream<String> lines){
        return  lines.flatMap(
                new FlatMapFunction<String, String>() {
                    @Override public Iterable<String> call(String x) {
                        return Arrays.asList(SPACE.split(x));
                    }
                });
    }

    /**
     * A function that returns key-value pairs (Tuple2&lt;K, V&gt;)
     * PairFunction<T, K, V>
     * @return
     */
    public static PairFunction<String, String, Integer> getPairFunction(){
        return new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String s) {
                return new Tuple2<>(s, 1);
            }
        };
    }

    /**
     * Function2<T1, T2, R>
     * A two-argument function that takes arguments of type T1 and T2 and returns an R.
     * @return
     */
    public static Function2<Integer, Integer, Integer> getReduceFunc(){
        return new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        };
    }
}
